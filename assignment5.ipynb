{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOAwc6ShxF9JxVR3JkmGacW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoominRho/ML_study_repo/blob/main/assignment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "Z-wtjOyeATV5",
        "outputId": "107f3fa8-42a6-4a2d-d54b-e6e95974ab20"
      },
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# dataset and transformation\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "# display images\n",
        "from torchvision import utils\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# utils\n",
        "import numpy as np\n",
        "from torchsummary import summary\n",
        "import time\n",
        "import copy\n",
        "torch.cuda.is_available()\n",
        "np.random.seed(20)\n",
        "\n",
        "train_raw = loadmat('./MyDrive/MyDrive/colab/train_32x32.mat')\n",
        "test_raw = loadmat('./MyDrive/MyDrive/colab/test_32x32.mat')\n",
        "train_images = np.array(train_raw['X'])\n",
        "test_images = np.array(test_raw['X'])\n",
        "train_labels = train_raw['y']\n",
        "test_labels = test_raw['y']\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)\n",
        "\n",
        "train_images = np.moveaxis(train_images, -1, 0)\n",
        "test_images = np.moveaxis(test_images, -1, 0)\n",
        "train_images = np.moveaxis(train_images, -1, 1)\n",
        "test_images = np.moveaxis(test_images, -1, 1)\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)\n",
        "\n",
        "train_images = torch.from_numpy(train_images)\n",
        "test_images = torch.from_numpy(test_images)\n",
        "train_labels = torch.from_numpy(train_labels)\n",
        "test_labels = torch.from_numpy(test_labels)\n",
        "\n",
        "print(train_images[1].shape)\n",
        "print(test_images[1].shape)\n",
        "print(len(train_images))\n",
        "print(len(test_images))\n",
        "\n",
        "# using FiveCrop, normalize, horizontal reflection\n",
        "train_transformer = transforms.Compose([\n",
        "                    transforms.Resize(256),\n",
        "                    transforms.FiveCrop(224),\n",
        "                    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
        "])\n",
        "\n",
        "train_images.transform = train_transformer\n",
        "train_labels.transform = train_transformer\n",
        "\n",
        "def show(imgs, y=None, color=True):\n",
        "    for i, img in enumerate(imgs):\n",
        "        npimg = img.numpy()\n",
        "        npimg_tr = np.transpose(npimg, (1, 2, 0))\n",
        "        plt.subplot(1, imgs.shape[0], i+1)\n",
        "        plt.imshow(npimg_tr)\n",
        "    \n",
        "    # plt.imshow(npimg_tr)\n",
        "    if y is not None:\n",
        "        plt.title('labels: ' + str(y))\n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# pick a random sample image\n",
        "rnd_inds = int(np.random.randint(0, len(train_images), 1))\n",
        "img = train_images[rnd_inds]\n",
        "print('images indices: ', rnd_inds)\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "show(img)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu102)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchvision) (3.7.4.3)\n",
            "(32, 32, 3, 73257)\n",
            "(32, 32, 3, 26032)\n",
            "(73257, 3, 32, 32)\n",
            "(26032, 3, 32, 32)\n",
            "torch.Size([3, 32, 32])\n",
            "torch.Size([3, 32, 32])\n",
            "73257\n",
            "26032\n",
            "images indices:  68268\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-afb388ae760f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-afb388ae760f>\u001b[0m in \u001b[0;36mshow\u001b[0;34m(imgs, y, color)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mnpimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mnpimg_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, axes)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \"\"\"\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transpose'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: axes don't match array"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x1440 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P61REM25yWN"
      },
      "source": [
        "# 새 섹션"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaZUTwaWcpCU",
        "outputId": "e1732359-2625-4097-b44d-9ebfb5374c60"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(root='./data', split='train', download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(root='./data', split='test', download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "#print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            #3 224 128\n",
        "            nn.Conv2d(3, 64, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            #64 112 64\n",
        "            nn.Conv2d(64, 128, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            #128 56 32\n",
        "            nn.Conv2d(128, 256, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            #256 28 16\n",
        "            nn.Conv2d(256, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            #512 14 8\n",
        "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        #512 7 4\n",
        "\n",
        "        self.avg_pool = nn.AvgPool2d(7)\n",
        "        #512 1 1\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "        \"\"\"\n",
        "        self.fc1 = nn.Linear(512*2*2,4096)\n",
        "        self.fc2 = nn.Linear(4096,4096)\n",
        "        self.fc3 = nn.Linear(4096,10)\n",
        "        \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #print(x.size())\n",
        "        features = self.conv(x)\n",
        "        #print(features.size())\n",
        "        x = self.avg_pool(features)\n",
        "        #print(avg_pool.size())\n",
        "        x = x.view(features.size(0), -1)\n",
        "        #print(flatten.size())\n",
        "        x = self.classifier(x)\n",
        "        #x = self.softmax(x)\n",
        "        return x, features\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net = Net()\n",
        "net = net.to(device)\n",
        "param = list(net.parameters())\n",
        "print(len(param))\n",
        "for i in param:\n",
        "    print(i.shape)\n",
        "#print(param[0].shape)\n",
        "classes =  ('1', '2', '3', '4', '5', '6', '7', '8', '9', '0')\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(net.parameters(),lr=0.00001)\n",
        "\n",
        "for epoch in range(100):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #print(inputs.shape)\n",
        "        #print(inputs.shape)  \n",
        "        # forward + backward + optimize\n",
        "        outputs,f = net(inputs)\n",
        "        #print(outputs.shape)\n",
        "        #print(labels.shape)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if(loss.item() > 1000):\n",
        "            print(loss.item())\n",
        "            for param in net.parameters():\n",
        "                print(param.data)\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 50 == 49:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 50))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs,_ = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
        "    \n",
        "\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs, f = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Using downloaded and verified file: ./data/train_32x32.mat\n",
            "Using downloaded and verified file: ./data/test_32x32.mat\n",
            "28\n",
            "torch.Size([64, 3, 3, 3])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 64, 3, 3])\n",
            "torch.Size([64])\n",
            "torch.Size([128, 64, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([128, 128, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([256, 128, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256])\n",
            "torch.Size([512, 256, 3, 3])\n",
            "torch.Size([512])\n",
            "torch.Size([512, 512, 3, 3])\n",
            "torch.Size([512])\n",
            "torch.Size([512, 512, 3, 3])\n",
            "torch.Size([512])\n",
            "torch.Size([512, 512, 3, 3])\n",
            "torch.Size([512])\n",
            "torch.Size([512, 512, 3, 3])\n",
            "torch.Size([512])\n",
            "torch.Size([512, 512, 3, 3])\n",
            "torch.Size([512])\n",
            "torch.Size([10, 512])\n",
            "torch.Size([10])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,    50] loss: 2.291\n",
            "[1,   100] loss: 2.237\n",
            "[1,   150] loss: 2.236\n",
            "[1,   200] loss: 2.244\n",
            "[1,   250] loss: 2.227\n",
            "[1,   300] loss: 2.239\n",
            "[1,   350] loss: 2.233\n",
            "[1,   400] loss: 2.234\n",
            "[1,   450] loss: 2.242\n",
            "[1,   500] loss: 2.239\n",
            "[1,   550] loss: 2.237\n",
            "[1,   600] loss: 2.235\n",
            "[1,   650] loss: 2.250\n",
            "[1,   700] loss: 2.240\n",
            "[1,   750] loss: 2.242\n",
            "[1,   800] loss: 2.243\n",
            "[1,   850] loss: 2.227\n",
            "[1,   900] loss: 2.240\n",
            "[1,   950] loss: 2.228\n",
            "[1,  1000] loss: 2.240\n",
            "[1,  1050] loss: 2.238\n",
            "[1,  1100] loss: 2.238\n",
            "[2,    50] loss: 2.248\n",
            "[2,   100] loss: 2.246\n",
            "[2,   150] loss: 2.233\n",
            "[2,   200] loss: 2.233\n",
            "[2,   250] loss: 2.240\n",
            "[2,   300] loss: 2.247\n",
            "[2,   350] loss: 2.244\n",
            "[2,   400] loss: 2.235\n",
            "[2,   450] loss: 2.231\n",
            "[2,   500] loss: 2.234\n",
            "[2,   550] loss: 2.226\n",
            "[2,   600] loss: 2.234\n",
            "[2,   650] loss: 2.251\n",
            "[2,   700] loss: 2.234\n",
            "[2,   750] loss: 2.236\n",
            "[2,   800] loss: 2.233\n",
            "[2,   850] loss: 2.228\n",
            "[2,   900] loss: 2.244\n",
            "[2,   950] loss: 2.247\n",
            "[2,  1000] loss: 2.218\n",
            "[2,  1050] loss: 2.237\n",
            "[2,  1100] loss: 2.241\n",
            "[3,    50] loss: 2.220\n",
            "[3,   100] loss: 2.228\n",
            "[3,   150] loss: 2.249\n",
            "[3,   200] loss: 2.244\n",
            "[3,   250] loss: 2.231\n",
            "[3,   300] loss: 2.254\n",
            "[3,   350] loss: 2.238\n",
            "[3,   400] loss: 2.238\n",
            "[3,   450] loss: 2.235\n",
            "[3,   500] loss: 2.227\n",
            "[3,   550] loss: 2.243\n",
            "[3,   600] loss: 2.251\n",
            "[3,   650] loss: 2.237\n",
            "[3,   700] loss: 2.238\n",
            "[3,   750] loss: 2.232\n",
            "[3,   800] loss: 2.248\n",
            "[3,   850] loss: 2.234\n",
            "[3,   900] loss: 2.229\n",
            "[3,   950] loss: 2.234\n",
            "[3,  1000] loss: 2.230\n",
            "[3,  1050] loss: 2.234\n",
            "[3,  1100] loss: 2.230\n",
            "[4,    50] loss: 2.234\n",
            "[4,   100] loss: 2.228\n",
            "[4,   150] loss: 2.228\n",
            "[4,   200] loss: 2.231\n",
            "[4,   250] loss: 2.243\n",
            "[4,   300] loss: 2.217\n",
            "[4,   350] loss: 2.227\n",
            "[4,   400] loss: 2.212\n",
            "[4,   450] loss: 2.223\n",
            "[4,   500] loss: 2.207\n",
            "[4,   550] loss: 2.198\n",
            "[4,   600] loss: 2.205\n",
            "[4,   650] loss: 2.191\n",
            "[4,   700] loss: 2.170\n",
            "[4,   750] loss: 2.161\n",
            "[4,   800] loss: 2.148\n",
            "[4,   850] loss: 2.131\n",
            "[4,   900] loss: 2.081\n",
            "[4,   950] loss: 2.068\n",
            "[4,  1000] loss: 2.029\n",
            "[4,  1050] loss: 2.007\n",
            "[4,  1100] loss: 1.994\n",
            "[5,    50] loss: 1.962\n",
            "[5,   100] loss: 1.933\n",
            "[5,   150] loss: 1.919\n",
            "[5,   200] loss: 1.899\n",
            "[5,   250] loss: 1.919\n",
            "[5,   300] loss: 1.889\n",
            "[5,   350] loss: 1.842\n",
            "[5,   400] loss: 1.838\n",
            "[5,   450] loss: 1.809\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}